{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Colab version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. set up environment on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Download full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL of the file you want to download\n",
    "# file_url = \"https://codeload.github.com/SaFoLab-WISC/JailBreakV_28K/zip/refs/heads/V0.2\"\n",
    "file_url = \"https://drive.usercontent.google.com/download?id=1ZrvSHklXiGYhpiVoxUH8FWc5k0fv2xVZ&export=download&authuser=0&confirm=t&uuid=0e44de7a-77d4-4b81-a953-1208d111221c&at=AN8xHooGPXhnTlQjNRWrPFLvCaSJ%3A1758342827507\"\n",
    "\n",
    "# Define the destination path in Google Drive\n",
    "# Make sure the folder 'Colab Notebooks/' exists in your Google Drive\n",
    "destination_path = \"\"\n",
    "\n",
    "# Use !wget to download the file directly to the specified path\n",
    "!wget \"{file_url}\" -P \"{destination_path}\" --no-check-certificate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 unzip full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define your paths ---\n",
    "\n",
    "# 1. output_path: Where the zip file is saved after download\n",
    "#    (The exact path depends on where you want it in 'My Drive')\n",
    "output_path = \"/content/drive/MyDrive/Colab Notebooks/JailBreakV_28K.zip\"\n",
    "\n",
    "# 2. extract_dir: The destination folder for the unzipped contents\n",
    "#    (Again, define where in 'My Drive' you want the files to go)\n",
    "extract_dir = \"/content/drive/MyDrive/Colab Notebooks/JailBreakV_28K\"\n",
    "\n",
    "# --- Execute the unzip command ---\n",
    "\n",
    "# The command uses the values stored in the Python variables above\n",
    "!unzip -q \"{output_path}\" -d \"{extract_dir}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re # Added regex import\n",
    "from PIL import Image\n",
    "from pydantic import BaseModel, Field, ValidationError # Added ValidationError import\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data\n",
    "base_data_path = \"/content/drive/My Drive/Colab Notebooks/JailBreakV_28K/JailBreakV_28k/\"\n",
    "df_raw = pd.read_csv(os.path.join(base_data_path, \"JailBreakV_28K.csv\"))\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process each image using Llava model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Using Llava model via HuggingfaceÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!!! Select A100 environment on Colab for best performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pydantic Schema and Prompt ---\n",
    "class ImageContent(BaseModel):\n",
    "    image_description: str = Field(description=\"A general summary of the image content.\")\n",
    "    visible_text: list[str] = Field(description=\"A list of all words or sentences that are visually present as text within the image.\")\n",
    "\n",
    "# UPDATED PROMPT: More strictly demands JSON output\n",
    "content_prompt = \"\"\"\n",
    "Analyze the image and generate a single, valid JSON object that strictly adheres to the following Python Pydantic schema:\n",
    "\n",
    "class ImageContent(BaseModel):\n",
    "    image_description: str = Field(description=\"A general summary of the image content, less than 25 words.\")\n",
    "    visible_text: list[str] = Field(description=\"A list of all words or sentences present as text in the image, less than 10 unique words.\")\n",
    "\n",
    "Ensure the output is ONLY the JSON object itself, without any introductory text or markdown wrappers like ```json.\n",
    "\"\"\"\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# --- Global Model and Processor Initialization ---\n",
    "\n",
    "# Check for GPU availability and use it\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == 'cpu':\n",
    "    print(\"WARNING: Running on CPU in Colab will be very slow. Ensure you have a GPU runtime enabled.\")\n",
    "\n",
    "# Load the LLaVA model and processor from Hugging Face Hub\n",
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"llava-hf/llava-1.5-7b-hf\",\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\" # Automatically handles model placement on GPU\n",
    ")\n",
    "\n",
    "print(\"Model and processor loaded successfully.\")\n",
    "# --------------------------------------------------------\n",
    "\n",
    "def extract_json_from_response(response_text):\n",
    "    \"\"\"\n",
    "    Attempts to extract a valid JSON string from potentially messy model output.\n",
    "    Handles responses wrapped in markdown blocks (```json ... ```) or raw JSON.\n",
    "    \"\"\"\n",
    "    # Regex to find JSON block wrapped in ```json or ```\n",
    "    match = re.search(r'```(?:json\\n)?(.*?)```', response_text, re.DOTALL)\n",
    "    if match:\n",
    "        json_str = match.group(1).strip()\n",
    "    else:\n",
    "        # Assume the entire response *might* be raw JSON\n",
    "        json_str = response_text.strip()\n",
    "\n",
    "    # Try to load the cleaned string as JSON\n",
    "    try:\n",
    "        json_object = json.loads(json_str)\n",
    "        # Re-dump to normalize the string format for Pydantic validation later\n",
    "        return json.dumps(json_object)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Failed to decode raw JSON string during extraction.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_image_with_llava_hf(image_path):\n",
    "    \"\"\"\n",
    "    Processes an image using the loaded HuggingFace LLaVA model via chat template.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"File not found: {image_path}. Skipping.\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        # Open the image using PIL\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": image},\n",
    "                    {\"type\": \"text\", \"text\": content_prompt}\n",
    "                ]\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        inputs = processor.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_tokens = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=300, # Increased tokens for safer JSON output\n",
    "                temperature=0.0,     # Deterministic output\n",
    "                pad_token_id=processor.tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        # Decode the generated tokens (skipping the input prompt part)\n",
    "        generated_tokens = output_tokens[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "        raw_response = processor.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "        # --- Use the robust JSON parser ---\n",
    "        json_string = extract_json_from_response(raw_response)\n",
    "\n",
    "        if json_string:\n",
    "            try:\n",
    "                details = ImageContent.model_validate_json(json_string)\n",
    "                return details.image_description, details.visible_text\n",
    "            except ValidationError as e:\n",
    "                print(f\"Pydantic validation failed for {image_path}. Error: {e}\")\n",
    "                print(f\"Problematic JSON: {json_string[:200]}...\")\n",
    "        else:\n",
    "            print(f\"Could not extract valid JSON from response for {image_path}.\")\n",
    "            print(f\"Raw response: {raw_response[:200]}...\")\n",
    "\n",
    "        # Fallback if any parsing/validation fails\n",
    "        return \"Parsing Error/Check Logs\", []\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred processing {image_path}: {e}\")\n",
    "        time.sleep(1)\n",
    "        return None, None\n",
    "\n",
    "# --- Main script with Caching Logic ---\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # !!! Update this path to where your data is located in Google Drive if necessary !!!\n",
    "    base_data_path = \"/content/drive/MyDrive/Colab Notebooks/JailBreakV_28K/JailBreakV_28k/\"\n",
    "\n",
    "    if not os.path.exists(base_data_path):\n",
    "        print(f\"Error: Base data path not found: {base_data_path}\")\n",
    "        exit()\n",
    "\n",
    "    df_raw = pd.read_csv(os.path.join(base_data_path, \"JailBreakV_28K.csv\"))\n",
    "\n",
    "    temp_cache_file = '/content/drive/MyDrive/Colab Notebooks/JailBreakV_28K/JailBreakV_28k/temp_image_llm_results.csv'\n",
    "    processed_results = []\n",
    "\n",
    "    print(\"------------------ Starting image processing with caching...\")\n",
    "\n",
    "    if os.path.exists(temp_cache_file):\n",
    "        cached_df = pd.read_csv(temp_cache_file)\n",
    "        cached_df = df_raw[~df_raw['image_path'].isin(cached_paths)]['image_path']\n",
    "        cached_paths_list = cached_df['img_path'].astype(str).tolist()\n",
    "        processed_results = cached_df.to_dict(orient='records')\n",
    "\n",
    "        print(f\"Loaded {len(processed_results)} cached results.\")\n",
    "        cached_paths = set(cached_paths_list)\n",
    "        images_to_process = df_raw[~df_raw['image_path'].isin(cached_paths)]['image_path'].tolist()\n",
    "    else:\n",
    "        images_to_process = df_raw['image_path'].tolist()\n",
    "\n",
    "    for i, image_path in enumerate(images_to_process):\n",
    "        print(f\"Processing {i+1}/{len(images_to_process)}: {image_path}...\")\n",
    "\n",
    "        full_image_path = os.path.join(base_data_path, image_path)\n",
    "\n",
    "        description, text_list = process_image_with_llava_hf(full_image_path)\n",
    "\n",
    "        if description is not None:\n",
    "            result_dict = {\n",
    "                'img_path': image_path,\n",
    "                'image_description': description,\n",
    "                'visible_text': json.dumps(text_list)\n",
    "            }\n",
    "            processed_results.append(result_dict)\n",
    "            pd.DataFrame(processed_results).to_csv(temp_cache_file, index=False)\n",
    "            print(f\"Successfully processed and cached: {image_path}\")\n",
    "\n",
    "    df_results = pd.DataFrame(processed_results)\n",
    "\n",
    "    df_final = pd.merge(df_raw, df_results, left_on='image_path', right_on='img_path', how='left')\n",
    "    df_final['visible_text'] = df_final['visible_text'].apply(lambda x: json.loads(x) if pd.notna(x) else [])\n",
    "\n",
    "    if 'img_path' in df_final.columns and 'image_path' in df_final.columns:\n",
    "        df_final = df_final.drop(columns=['img_path'])\n",
    "\n",
    "    print(\"\\n--- Final Merged DataFrame ---\")\n",
    "    print(df_final.head())\n",
    "    print(f\"Total processed images: {len(df_final)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 (Optional) using Llava via model served on Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### install Colab extension to run terminal in Colab (to run Ollama) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install colab-xterm\n",
    "%load_ext colabxterm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### install Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://ollama.ai/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open 2 terminals to run Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%xterm` not found.\n"
     ]
    }
   ],
   "source": [
    "%xterm\n",
    "# then use command as below:\n",
    "# ollama serve &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%xterm\n",
    "# then use command as below:\n",
    "# ollama pull llava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install ollama python library\n",
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear CUDA Cache to free GPU RAM\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import time\n",
    "import json\n",
    "import ollama\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# --- Pydantic Schema and Prompt (from original code) ---\n",
    "class ImageContent(BaseModel):\n",
    "    image_description: str = Field(description=\"A general summary of the image content.\")\n",
    "    visible_text: list[str] = Field(description=\"A list of all words or sentences that are visually present as text within the image.\")\n",
    "\n",
    "content_prompt = \"\"\"\n",
    "Using the image, provide a detailed description. Confine the response to one sentence with less than 25 words, highlighting the main subject's appearance and environment. Example: Input Image: [image of a brown dog] Output: A small, shaggy brown dog with a red collar is sitting on a green lawn with a blue ball nearby.\n",
    "\"\"\"\n",
    "# --------------------------------------------------------\n",
    "\n",
    "def resize_image_if_needed(image_path, max_width=600, max_height=400):\n",
    "    \"\"\"\n",
    "    Resizes an image proportionally if it's larger than max_width or max_height,\n",
    "    and returns the image data as bytes ready for the Ollama API.\n",
    "    Forces JPEG compression for speed and payload size reduction.\n",
    "    \"\"\"\n",
    "    with Image.open(image_path) as img:\n",
    "        width, height = img.size\n",
    "\n",
    "        if width > max_width or height > max_height:\n",
    "            # Calculate new dimensions while maintaining aspect ratio\n",
    "            img.thumbnail((max_width, max_height), Image.Resampling.LANCZOS)\n",
    "            print(f\"Resized image from {width}x{height} to {img.size[0]}x{img.size[1]}\")\n",
    "\n",
    "        # Convert image to bytes in memory as JPEG for efficiency\n",
    "        img_byte_arr = io.BytesIO()\n",
    "        # Save as JPEG with moderate quality (85 is good balance of size/quality)\n",
    "        img.save(img_byte_arr, format='JPEG', quality=85)\n",
    "        return img_byte_arr.getvalue()\n",
    "\n",
    "def process_image_with_llava(image_path):\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"File not found: {image_path}. Skipping.\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        # Get the resized image bytes\n",
    "        image_bytes = resize_image_if_needed(image_path)\n",
    "\n",
    "        response = ollama.chat(\n",
    "            model='llava',\n",
    "            messages=[{\n",
    "                'role': 'user',\n",
    "                'content': content_prompt,\n",
    "                'images': [image_bytes]\n",
    "            }],\n",
    "            format=ImageContent.model_json_schema(),\n",
    "            options={\n",
    "                \"temperature\": 0.0,\n",
    "                # Explicitly set CPU options for best performance on CPU-only setup\n",
    "                \"num_threads\": os.cpu_count(), # Use all available CPU cores\n",
    "                # \"num_ctx\": 2048 # Adjust if you have memory issues, default is often fine\n",
    "            }\n",
    "        )\n",
    "\n",
    "        json_output_str = response['message']['content']\n",
    "        details = ImageContent.model_validate_json(json_output_str)\n",
    "\n",
    "        return details.image_description, details.visible_text\n",
    "\n",
    "    except ollama.ResponseError as e:\n",
    "        print(f\"Ollama API error for {image_path}: {e}\")\n",
    "        # If Ollama server is overloaded, waiting and retrying might help\n",
    "        time.sleep(10)\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred processing {image_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# --- Main script with Caching Logic (Remains mostly the same but added imports) ---\n",
    "if __name__ == '__main__':\n",
    "    # Make sure Ollama server is running (e.g., 'ollama run llava' in your terminal)\n",
    "\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # !!! Update this path to where your data is located in Google Drive !!!\n",
    "    base_data_path = \"/content/drive/MyDrive/Colab Notebooks/JailBreakV_28K/JailBreakV_28k/\"\n",
    "\n",
    "    if not os.path.exists(base_data_path):\n",
    "        print(f\"Error: Base data path not found: {base_data_path}\")\n",
    "        exit()\n",
    "\n",
    "    df_raw = pd.read_csv(os.path.join(base_data_path, \"JailBreakV_28K.csv\"))\n",
    "\n",
    "    temp_cache_file = 'temp_image_llm_results.csv'\n",
    "    processed_results = []\n",
    "\n",
    "    print(\"------------------ Starting image processing with caching...\")\n",
    "    print(f\"Detected {os.cpu_count()} CPU cores available for Ollama.\")\n",
    "\n",
    "    # Load existing cache if it exists to resume processing\n",
    "    if os.path.exists(temp_cache_file):\n",
    "        cached_df = pd.read_csv(temp_cache_file)\n",
    "        # Ensure column type consistency before set operations\n",
    "        cached_paths_list = cached_df['img_path'].astype(str).tolist()\n",
    "        processed_results = cached_df.to_dict(orient='records')\n",
    "        print(f\"Loaded {len(processed_results)} cached results.\")\n",
    "\n",
    "        # Determine which images still need processing\n",
    "        cached_paths = set(cached_paths_list)\n",
    "        images_to_process = df_raw[~df_raw['image_path'].isin(cached_paths)]['image_path'].tolist()\n",
    "    else:\n",
    "        images_to_process = df_raw['image_path'].tolist()\n",
    "\n",
    "    # Process remaining images\n",
    "    for i, image_path in enumerate(images_to_process):\n",
    "        print(f\"Processing {i+1}/{len(images_to_process)}: {image_path}...\")\n",
    "        # Assuming the relative path is correct based on your previous code\n",
    "        full_image_path = base_data_path + image_path\n",
    "        description, text_list = process_image_with_llava(full_image_path)\n",
    "\n",
    "        if description is not None:\n",
    "            result_dict = {\n",
    "                'img_path': image_path,\n",
    "                'image_description': description,\n",
    "                'visible_text': json.dumps(text_list)\n",
    "            }\n",
    "            processed_results.append(result_dict)\n",
    "            # Save intermittently\n",
    "            pd.DataFrame(processed_results).to_csv(temp_cache_file, index=False)\n",
    "            print(f\"Successfully processed and cached: {image_path}\")\n",
    "\n",
    "    df_results = pd.DataFrame(processed_results)\n",
    "    df_final = pd.merge(df_raw, df_results, on='img_path', how='left')\n",
    "    df_final['visible_text'] = df_final['visible_text'].apply(lambda x: json.loads(x) if pd.notna(x) else [])\n",
    "    print(\"\\n--- Final Merged DataFrame ---\")\n",
    "    print(df_final.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
